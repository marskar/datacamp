# Creating Robust Python Projects

## Chapter 1: Python project principles

* Lesson 1.1 - Python modules
    * Learning objective: Understand modularization, i.e. why split up code into modules? Python `import` statement, code reusability, avoiding monolith programs, e.g. a single Jupyter notebook with all code and documentation.
* Lesson 1.2 - Python packages
    * Learning objective: Understand packaging, i.e. why distribute data science projects in the form of packages? Sharing code, reproducibility, projects will not gain traction unless the code is easily available and well documented, popular projects can inspire others to contribute.
* Lesson 1.3 - Python ecosystem
    * Learning objective: Understand the Python packaging ecosystem, i.e. how can my code fit into the grander scheme of things? PyPI and pip (with a nod to conda-forge and conda), dependency management (requirements.txt), virtual environments (pipenv)

## Chapter 2: Beyond `.py` and `.ipynb`: Python projects are more than just code files!

* Lesson 2.1 - Directed Acyclic Graphs (DAGs)
  * Learning objective: A data analysis flows from start to finish and thus can be conceptualized as a directed acyclic graph (DAG). Use `networkx` to create a plot of [data cleaning](https://github.com/marskar/nhanes/blob/4022bd25b4baa82af56f25f6b02fd7759b1ee88a/kod/r-nhanes-cleaning.py) and [analysis](https://github.com/marskar/nhanes/blob/4022bd25b4baa82af56f25f6b02fd7759b1ee88a/kod/r-nhanes-analysis.py).
* Lesson 2.2 - Python project templates
    * Learning objective: Understand the structure and individual components of a typical Python project. To get started quickly, use a [cookiecutter](https://drivendata.github.io/cookiecutter-data-science/) template such as the one below:

```
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
```
* Lesson 2.3 - 

* Lesson 
  * What type of models are appropriate?
* Lesson 1.3 - Training and evaluating models
  * Splitting data into training and testing sets
  * Training a model
  * Evaluating the model

## Chapter 2: Get out the vote

* Lesson 2.1 - Voter attitudes and turnout
  * Exploratory data analysis
  * What types of models are appropriate?
* Lesson 2.2 - Training models
  * Logistic regression
  * glmnet
  * Random forest
* Lesson 2.3 - Evaluating your models
  * ROC and AUC

## Chapter 3: Essential copying and pasting from Stack Overflow

* Lesson 3.1 - Stack Overflow Developer Survey
  * Exploratory data analysis
  * What types of models are appropriate?
* Lesson 3.2 - Imputation for missing values
  * Preparing for modeling
* Lesson 3.3 - Training models
  * Logistic regression
  * Random forest
* Lesson 3.4 - Evaluating your models
  * ROC and AUC

## Chapter 4: But what do the nuns think?

* Lesson 4.1 - Survey of Catholic nuns in 1963
  * Exploratory data analysis
  * What types of models are appropriate?
* Lesson 4.2 - Highly correlated variables
  * Preparing for modeling
* Lesson 4.3 - Training models
  * Linear regression
  * Random forest
  * (maybe will try gbm?)
* Lesson 4.4 - Evaluating your models
  * RMSE


